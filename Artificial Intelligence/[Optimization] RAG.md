# Retrieval Augmented Generation Overview

RAG is the process of optimizing the output of a LLM by allowing it to reference an authoritative knowledge base outside of its training data sources before generating a response

* Extends the already powerful capabilities of LLMs to specific domains or an organization's internal knowledge base all without the need to retrain the model
* A cost-effective approach to improviing LLM output so it remains relevant, accurate, and useful
* Allows LLMs to answer questions in various contexts by cross-referencing authoritative knowledge sources since LLM training data is static and introduces a cut-off date on the knowledge it has
* Redirects LLMs to retrieve relevant information from relevant authoritative and pre-determined knowledge sources

<br>

# Known LLM Challenges

* Presenting false information when it doesn't have the answer
* Presenting out-of-date or generic information when the user expects a specific and current response
* Creating a response from non-authoritative sources
* Creating inaccurate responses due to terminology confusion

<br>

# RAG Benefits

* A cost-effective approach to introducing new data to a foundational model (*The computational and financial costs of retraining foundational models for organizations or domain-specific information are high*)